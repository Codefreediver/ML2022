{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "optimization freedom:\n",
    "(1) Batch Size\n",
    "(2) Model\n",
    "(3) num of epoch\n",
    "(4) learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_11.npy → training data (# of training frames, 11 x feature dim)\n",
    "train_label_11.npy → framewise phoneme label (0-38)\n",
    "test_11.npy → testing data (# of testing frames, 11 x feature dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Size of training data: (1229932, 429)\n",
      "Size of testing data: (451552, 429)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Loading data ...')\n",
    "\n",
    "data_root=\"C:\\\\Users\\\\Michael\\\\Desktop\\\\PYTHON\\\\Machine Learning\\\\HW2\\\\timit_11\\\\\"\n",
    "train = np.load(data_root + 'train_11.npy')\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "test = np.load(data_root + 'test_11.npy')\n",
    "\n",
    "print('Size of training data: {}'.format(train.shape))\n",
    "print('Size of testing data: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create DataSet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TIMITDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = torch.from_numpy(X).float()\n",
    "        if y is not None:\n",
    "            y = y.astype(int)\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliy Train and Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: (983945, 429)\n",
      "Size of validation set: (245987, 429)\n"
     ]
    }
   ],
   "source": [
    "VAL_RATIO = 0.2\n",
    "\n",
    "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
    "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
    "print('Size of training set: {}'.format(train_x.shape))\n",
    "print('Size of validation set: {}'.format(val_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = TIMITDataset(train_x, train_y)\n",
    "val_set = TIMITDataset(val_x, val_y)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False) # Why shuffle the training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del train, train_label, train_x, train_y, val_x, val_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(429, 2048), # 1\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 2048), # 2\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 2048), # 2\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048,1024), # 3\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512), # 4\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256), # 5\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 39)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check device\n",
    "def get_device():\n",
    "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "same_seeds(0)\n",
    "\n",
    "# get device \n",
    "device = get_device()\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "# training parameters\n",
    "num_epoch = 250             # number of training epoch\n",
    "learning_rate = 1e-4       # learning rate\n",
    "l2 = 1e-4\n",
    "\n",
    "# the path where checkpoint saved\n",
    "model_path = './model.ckpt'\n",
    "\n",
    "# create model, define a loss function, and optimizer\n",
    "model = Classifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/250] Train Acc: 0.391858 Loss: 2.202476 | Val Acc: 0.553806 loss: 1.497888\n",
      "saving model with acc 0.554\n",
      "[002/250] Train Acc: 0.527697 Loss: 1.604127 | Val Acc: 0.620273 loss: 1.249955\n",
      "saving model with acc 0.620\n",
      "[003/250] Train Acc: 0.570423 Loss: 1.440136 | Val Acc: 0.651681 loss: 1.137280\n",
      "saving model with acc 0.652\n",
      "[004/250] Train Acc: 0.596523 Loss: 1.343338 | Val Acc: 0.667641 loss: 1.072795\n",
      "saving model with acc 0.668\n",
      "[005/250] Train Acc: 0.613453 Loss: 1.279361 | Val Acc: 0.680760 loss: 1.026321\n",
      "saving model with acc 0.681\n",
      "[006/250] Train Acc: 0.628411 Loss: 1.227591 | Val Acc: 0.690463 loss: 0.991680\n",
      "saving model with acc 0.690\n",
      "[007/250] Train Acc: 0.637939 Loss: 1.188015 | Val Acc: 0.697663 loss: 0.963799\n",
      "saving model with acc 0.698\n",
      "[008/250] Train Acc: 0.646942 Loss: 1.155512 | Val Acc: 0.703261 loss: 0.941507\n",
      "saving model with acc 0.703\n",
      "[009/250] Train Acc: 0.654709 Loss: 1.127497 | Val Acc: 0.708375 loss: 0.922601\n",
      "saving model with acc 0.708\n",
      "[010/250] Train Acc: 0.661369 Loss: 1.101995 | Val Acc: 0.712960 loss: 0.906237\n",
      "saving model with acc 0.713\n",
      "[011/250] Train Acc: 0.667672 Loss: 1.079980 | Val Acc: 0.717635 loss: 0.891843\n",
      "saving model with acc 0.718\n",
      "[012/250] Train Acc: 0.672561 Loss: 1.061114 | Val Acc: 0.721319 loss: 0.876756\n",
      "saving model with acc 0.721\n",
      "[013/250] Train Acc: 0.677016 Loss: 1.043272 | Val Acc: 0.724351 loss: 0.865931\n",
      "saving model with acc 0.724\n",
      "[014/250] Train Acc: 0.682069 Loss: 1.026489 | Val Acc: 0.725331 loss: 0.861073\n",
      "saving model with acc 0.725\n",
      "[015/250] Train Acc: 0.686123 Loss: 1.011395 | Val Acc: 0.728412 loss: 0.849667\n",
      "saving model with acc 0.728\n",
      "[016/250] Train Acc: 0.690050 Loss: 0.996664 | Val Acc: 0.731120 loss: 0.840487\n",
      "saving model with acc 0.731\n",
      "[017/250] Train Acc: 0.693629 Loss: 0.983852 | Val Acc: 0.734144 loss: 0.830833\n",
      "saving model with acc 0.734\n",
      "[018/250] Train Acc: 0.697490 Loss: 0.970404 | Val Acc: 0.735815 loss: 0.824275\n",
      "saving model with acc 0.736\n",
      "[019/250] Train Acc: 0.700770 Loss: 0.959017 | Val Acc: 0.737523 loss: 0.820161\n",
      "saving model with acc 0.738\n",
      "[020/250] Train Acc: 0.703580 Loss: 0.946975 | Val Acc: 0.738572 loss: 0.813815\n",
      "saving model with acc 0.739\n",
      "[021/250] Train Acc: 0.707374 Loss: 0.934994 | Val Acc: 0.741031 loss: 0.805237\n",
      "saving model with acc 0.741\n",
      "[022/250] Train Acc: 0.709918 Loss: 0.923832 | Val Acc: 0.742572 loss: 0.802642\n",
      "saving model with acc 0.743\n",
      "[023/250] Train Acc: 0.712579 Loss: 0.914924 | Val Acc: 0.743393 loss: 0.798100\n",
      "saving model with acc 0.743\n",
      "[024/250] Train Acc: 0.715368 Loss: 0.903352 | Val Acc: 0.745568 loss: 0.791837\n",
      "saving model with acc 0.746\n",
      "[025/250] Train Acc: 0.718483 Loss: 0.894087 | Val Acc: 0.746084 loss: 0.788990\n",
      "saving model with acc 0.746\n",
      "[026/250] Train Acc: 0.721119 Loss: 0.884362 | Val Acc: 0.747462 loss: 0.785314\n",
      "saving model with acc 0.747\n",
      "[027/250] Train Acc: 0.723079 Loss: 0.877352 | Val Acc: 0.747901 loss: 0.782783\n",
      "saving model with acc 0.748\n",
      "[028/250] Train Acc: 0.725647 Loss: 0.868430 | Val Acc: 0.749296 loss: 0.779009\n",
      "saving model with acc 0.749\n",
      "[029/250] Train Acc: 0.727506 Loss: 0.860121 | Val Acc: 0.750536 loss: 0.775544\n",
      "saving model with acc 0.751\n",
      "[030/250] Train Acc: 0.729803 Loss: 0.851874 | Val Acc: 0.750674 loss: 0.773587\n",
      "saving model with acc 0.751\n",
      "[031/250] Train Acc: 0.732322 Loss: 0.843497 | Val Acc: 0.750767 loss: 0.774386\n",
      "saving model with acc 0.751\n",
      "[032/250] Train Acc: 0.734093 Loss: 0.837224 | Val Acc: 0.752824 loss: 0.769034\n",
      "saving model with acc 0.753\n",
      "[033/250] Train Acc: 0.735379 Loss: 0.830760 | Val Acc: 0.753235 loss: 0.764947\n",
      "saving model with acc 0.753\n",
      "[034/250] Train Acc: 0.737985 Loss: 0.824048 | Val Acc: 0.753861 loss: 0.764081\n",
      "saving model with acc 0.754\n",
      "[035/250] Train Acc: 0.739775 Loss: 0.816977 | Val Acc: 0.754698 loss: 0.760975\n",
      "saving model with acc 0.755\n",
      "[036/250] Train Acc: 0.741320 Loss: 0.811076 | Val Acc: 0.755093 loss: 0.760169\n",
      "saving model with acc 0.755\n",
      "[037/250] Train Acc: 0.742743 Loss: 0.805389 | Val Acc: 0.754784 loss: 0.761131\n",
      "[038/250] Train Acc: 0.744279 Loss: 0.800799 | Val Acc: 0.755207 loss: 0.758973\n",
      "saving model with acc 0.755\n",
      "[039/250] Train Acc: 0.746022 Loss: 0.794195 | Val Acc: 0.756162 loss: 0.758632\n",
      "saving model with acc 0.756\n",
      "[040/250] Train Acc: 0.747805 Loss: 0.787488 | Val Acc: 0.757093 loss: 0.755548\n",
      "saving model with acc 0.757\n",
      "[041/250] Train Acc: 0.748947 Loss: 0.784011 | Val Acc: 0.756971 loss: 0.755844\n",
      "[042/250] Train Acc: 0.750376 Loss: 0.778695 | Val Acc: 0.758382 loss: 0.750427\n",
      "saving model with acc 0.758\n",
      "[043/250] Train Acc: 0.751771 Loss: 0.774342 | Val Acc: 0.757780 loss: 0.753710\n",
      "[044/250] Train Acc: 0.752944 Loss: 0.768694 | Val Acc: 0.758666 loss: 0.752408\n",
      "saving model with acc 0.759\n",
      "[045/250] Train Acc: 0.754016 Loss: 0.765577 | Val Acc: 0.758296 loss: 0.753018\n",
      "[046/250] Train Acc: 0.755768 Loss: 0.759820 | Val Acc: 0.759703 loss: 0.748566\n",
      "saving model with acc 0.760\n",
      "[047/250] Train Acc: 0.756811 Loss: 0.755279 | Val Acc: 0.759776 loss: 0.749117\n",
      "saving model with acc 0.760\n",
      "[048/250] Train Acc: 0.758254 Loss: 0.750692 | Val Acc: 0.760386 loss: 0.748041\n",
      "saving model with acc 0.760\n",
      "[049/250] Train Acc: 0.758740 Loss: 0.749259 | Val Acc: 0.759975 loss: 0.747591\n",
      "[050/250] Train Acc: 0.759819 Loss: 0.743281 | Val Acc: 0.759861 loss: 0.749854\n",
      "[051/250] Train Acc: 0.760595 Loss: 0.740974 | Val Acc: 0.760109 loss: 0.747643\n",
      "[052/250] Train Acc: 0.761071 Loss: 0.736880 | Val Acc: 0.760935 loss: 0.747469\n",
      "saving model with acc 0.761\n",
      "[053/250] Train Acc: 0.762637 Loss: 0.732980 | Val Acc: 0.760825 loss: 0.746564\n",
      "[054/250] Train Acc: 0.763819 Loss: 0.728823 | Val Acc: 0.760329 loss: 0.748471\n",
      "[055/250] Train Acc: 0.765017 Loss: 0.726284 | Val Acc: 0.760898 loss: 0.747439\n",
      "[056/250] Train Acc: 0.765730 Loss: 0.722490 | Val Acc: 0.761500 loss: 0.744356\n",
      "saving model with acc 0.761\n",
      "[057/250] Train Acc: 0.766845 Loss: 0.719850 | Val Acc: 0.761768 loss: 0.743673\n",
      "saving model with acc 0.762\n",
      "[058/250] Train Acc: 0.767447 Loss: 0.717484 | Val Acc: 0.762093 loss: 0.744111\n",
      "saving model with acc 0.762\n",
      "[059/250] Train Acc: 0.767839 Loss: 0.714426 | Val Acc: 0.762016 loss: 0.743827\n",
      "[060/250] Train Acc: 0.769373 Loss: 0.710940 | Val Acc: 0.762715 loss: 0.740973\n",
      "saving model with acc 0.763\n",
      "[061/250] Train Acc: 0.769322 Loss: 0.709830 | Val Acc: 0.763427 loss: 0.739933\n",
      "saving model with acc 0.763\n",
      "[062/250] Train Acc: 0.771232 Loss: 0.705625 | Val Acc: 0.761922 loss: 0.744792\n",
      "[063/250] Train Acc: 0.771648 Loss: 0.702560 | Val Acc: 0.762504 loss: 0.744120\n",
      "[064/250] Train Acc: 0.771653 Loss: 0.700262 | Val Acc: 0.762422 loss: 0.742848\n",
      "[065/250] Train Acc: 0.772848 Loss: 0.697048 | Val Acc: 0.763914 loss: 0.739885\n",
      "saving model with acc 0.764\n",
      "[066/250] Train Acc: 0.773637 Loss: 0.695266 | Val Acc: 0.762597 loss: 0.743051\n",
      "[067/250] Train Acc: 0.774215 Loss: 0.691770 | Val Acc: 0.762666 loss: 0.745191\n",
      "[068/250] Train Acc: 0.774781 Loss: 0.690435 | Val Acc: 0.763101 loss: 0.740838\n",
      "[069/250] Train Acc: 0.775199 Loss: 0.688739 | Val Acc: 0.763113 loss: 0.741776\n",
      "[070/250] Train Acc: 0.775689 Loss: 0.686067 | Val Acc: 0.763317 loss: 0.740160\n",
      "[071/250] Train Acc: 0.776720 Loss: 0.683222 | Val Acc: 0.762695 loss: 0.744992\n",
      "[072/250] Train Acc: 0.777243 Loss: 0.681469 | Val Acc: 0.763630 loss: 0.741104\n",
      "[073/250] Train Acc: 0.778058 Loss: 0.679014 | Val Acc: 0.763666 loss: 0.742271\n",
      "[074/250] Train Acc: 0.778403 Loss: 0.677132 | Val Acc: 0.764565 loss: 0.741615\n",
      "saving model with acc 0.765\n",
      "[075/250] Train Acc: 0.778910 Loss: 0.675760 | Val Acc: 0.763882 loss: 0.740543\n",
      "[076/250] Train Acc: 0.779736 Loss: 0.672714 | Val Acc: 0.764008 loss: 0.742115\n",
      "[077/250] Train Acc: 0.780464 Loss: 0.670278 | Val Acc: 0.764248 loss: 0.742950\n",
      "[078/250] Train Acc: 0.781115 Loss: 0.669140 | Val Acc: 0.764069 loss: 0.742052\n",
      "[079/250] Train Acc: 0.781362 Loss: 0.666430 | Val Acc: 0.764435 loss: 0.741365\n",
      "[080/250] Train Acc: 0.781606 Loss: 0.665579 | Val Acc: 0.763483 loss: 0.743121\n",
      "[081/250] Train Acc: 0.782333 Loss: 0.663321 | Val Acc: 0.764427 loss: 0.741067\n",
      "[082/250] Train Acc: 0.782895 Loss: 0.662836 | Val Acc: 0.764857 loss: 0.741138\n",
      "saving model with acc 0.765\n",
      "[083/250] Train Acc: 0.783897 Loss: 0.659025 | Val Acc: 0.765146 loss: 0.739470\n",
      "saving model with acc 0.765\n",
      "[084/250] Train Acc: 0.783724 Loss: 0.659149 | Val Acc: 0.765451 loss: 0.741440\n",
      "saving model with acc 0.765\n",
      "[085/250] Train Acc: 0.784279 Loss: 0.657011 | Val Acc: 0.765236 loss: 0.741363\n",
      "[086/250] Train Acc: 0.784977 Loss: 0.654890 | Val Acc: 0.765427 loss: 0.741342\n",
      "[087/250] Train Acc: 0.785152 Loss: 0.654710 | Val Acc: 0.764219 loss: 0.742924\n",
      "[088/250] Train Acc: 0.785626 Loss: 0.652910 | Val Acc: 0.764118 loss: 0.743964\n",
      "[089/250] Train Acc: 0.786490 Loss: 0.650386 | Val Acc: 0.765752 loss: 0.739647\n",
      "saving model with acc 0.766\n",
      "[090/250] Train Acc: 0.786923 Loss: 0.648569 | Val Acc: 0.764772 loss: 0.744043\n",
      "[091/250] Train Acc: 0.787183 Loss: 0.647334 | Val Acc: 0.764797 loss: 0.741592\n",
      "[092/250] Train Acc: 0.787983 Loss: 0.644323 | Val Acc: 0.766370 loss: 0.739943\n",
      "saving model with acc 0.766\n",
      "[093/250] Train Acc: 0.788182 Loss: 0.643517 | Val Acc: 0.765110 loss: 0.740728\n",
      "[094/250] Train Acc: 0.788420 Loss: 0.643378 | Val Acc: 0.765882 loss: 0.740651\n",
      "[095/250] Train Acc: 0.788599 Loss: 0.641613 | Val Acc: 0.766362 loss: 0.740021\n",
      "[096/250] Train Acc: 0.788787 Loss: 0.640147 | Val Acc: 0.766162 loss: 0.739830\n",
      "[097/250] Train Acc: 0.789772 Loss: 0.639126 | Val Acc: 0.766089 loss: 0.739893\n",
      "[098/250] Train Acc: 0.789788 Loss: 0.638459 | Val Acc: 0.766256 loss: 0.739527\n",
      "[099/250] Train Acc: 0.790293 Loss: 0.635444 | Val Acc: 0.765898 loss: 0.741987\n",
      "[100/250] Train Acc: 0.790862 Loss: 0.634838 | Val Acc: 0.765122 loss: 0.743696\n",
      "[101/250] Train Acc: 0.790544 Loss: 0.633714 | Val Acc: 0.766008 loss: 0.739566\n",
      "[102/250] Train Acc: 0.790779 Loss: 0.633678 | Val Acc: 0.766142 loss: 0.741641\n",
      "[103/250] Train Acc: 0.791583 Loss: 0.631112 | Val Acc: 0.766077 loss: 0.740097\n",
      "[104/250] Train Acc: 0.791610 Loss: 0.630462 | Val Acc: 0.766142 loss: 0.740590\n",
      "[105/250] Train Acc: 0.792687 Loss: 0.628299 | Val Acc: 0.766244 loss: 0.740519\n",
      "[106/250] Train Acc: 0.792258 Loss: 0.627572 | Val Acc: 0.766553 loss: 0.740508\n",
      "saving model with acc 0.767\n",
      "[107/250] Train Acc: 0.793064 Loss: 0.626807 | Val Acc: 0.765008 loss: 0.741163\n",
      "[108/250] Train Acc: 0.793115 Loss: 0.625498 | Val Acc: 0.765914 loss: 0.742070\n",
      "[109/250] Train Acc: 0.793693 Loss: 0.623343 | Val Acc: 0.765825 loss: 0.742111\n",
      "[110/250] Train Acc: 0.793900 Loss: 0.622514 | Val Acc: 0.766394 loss: 0.741544\n",
      "[111/250] Train Acc: 0.793595 Loss: 0.623303 | Val Acc: 0.766333 loss: 0.742563\n",
      "[112/250] Train Acc: 0.794389 Loss: 0.621272 | Val Acc: 0.766333 loss: 0.741924\n",
      "[113/250] Train Acc: 0.794610 Loss: 0.620457 | Val Acc: 0.766915 loss: 0.741624\n",
      "saving model with acc 0.767\n",
      "[114/250] Train Acc: 0.795238 Loss: 0.618787 | Val Acc: 0.766244 loss: 0.742820\n",
      "[115/250] Train Acc: 0.795198 Loss: 0.618820 | Val Acc: 0.766134 loss: 0.739627\n",
      "[116/250] Train Acc: 0.795881 Loss: 0.617343 | Val Acc: 0.765984 loss: 0.743571\n",
      "[117/250] Train Acc: 0.795820 Loss: 0.616726 | Val Acc: 0.766488 loss: 0.741910\n",
      "[118/250] Train Acc: 0.795763 Loss: 0.615481 | Val Acc: 0.766541 loss: 0.741292\n",
      "[119/250] Train Acc: 0.795935 Loss: 0.614988 | Val Acc: 0.767215 loss: 0.738397\n",
      "saving model with acc 0.767\n",
      "[120/250] Train Acc: 0.797069 Loss: 0.613510 | Val Acc: 0.766305 loss: 0.742961\n",
      "[121/250] Train Acc: 0.797163 Loss: 0.612651 | Val Acc: 0.766992 loss: 0.740853\n",
      "[122/250] Train Acc: 0.797155 Loss: 0.610958 | Val Acc: 0.766626 loss: 0.742453\n",
      "[123/250] Train Acc: 0.797532 Loss: 0.610625 | Val Acc: 0.765817 loss: 0.745388\n",
      "[124/250] Train Acc: 0.798035 Loss: 0.608977 | Val Acc: 0.765683 loss: 0.744487\n",
      "[125/250] Train Acc: 0.797724 Loss: 0.609308 | Val Acc: 0.766780 loss: 0.743113\n",
      "[126/250] Train Acc: 0.798429 Loss: 0.607887 | Val Acc: 0.766658 loss: 0.743240\n",
      "[127/250] Train Acc: 0.798928 Loss: 0.607053 | Val Acc: 0.766471 loss: 0.744174\n",
      "[128/250] Train Acc: 0.799569 Loss: 0.604716 | Val Acc: 0.766776 loss: 0.744317\n",
      "[129/250] Train Acc: 0.798814 Loss: 0.606078 | Val Acc: 0.766662 loss: 0.741638\n",
      "[130/250] Train Acc: 0.799202 Loss: 0.605395 | Val Acc: 0.767768 loss: 0.743909\n",
      "saving model with acc 0.768\n",
      "[131/250] Train Acc: 0.799704 Loss: 0.603766 | Val Acc: 0.767439 loss: 0.743615\n",
      "[132/250] Train Acc: 0.800165 Loss: 0.602616 | Val Acc: 0.766939 loss: 0.743349\n",
      "[133/250] Train Acc: 0.799727 Loss: 0.601666 | Val Acc: 0.766931 loss: 0.745520\n",
      "[134/250] Train Acc: 0.799956 Loss: 0.601495 | Val Acc: 0.767077 loss: 0.743427\n",
      "[135/250] Train Acc: 0.800424 Loss: 0.600066 | Val Acc: 0.767256 loss: 0.743182\n",
      "[136/250] Train Acc: 0.801206 Loss: 0.599576 | Val Acc: 0.767329 loss: 0.744262\n",
      "[137/250] Train Acc: 0.801079 Loss: 0.598437 | Val Acc: 0.766565 loss: 0.743600\n",
      "[138/250] Train Acc: 0.801197 Loss: 0.597686 | Val Acc: 0.767036 loss: 0.744515\n",
      "[139/250] Train Acc: 0.802076 Loss: 0.596101 | Val Acc: 0.767049 loss: 0.746788\n",
      "[140/250] Train Acc: 0.801806 Loss: 0.597195 | Val Acc: 0.767484 loss: 0.744933\n",
      "[141/250] Train Acc: 0.801662 Loss: 0.597024 | Val Acc: 0.767553 loss: 0.745014\n",
      "[142/250] Train Acc: 0.801783 Loss: 0.595745 | Val Acc: 0.767187 loss: 0.744018\n",
      "[143/250] Train Acc: 0.802003 Loss: 0.594689 | Val Acc: 0.767341 loss: 0.743578\n",
      "[144/250] Train Acc: 0.802444 Loss: 0.593514 | Val Acc: 0.766382 loss: 0.743123\n",
      "[145/250] Train Acc: 0.802156 Loss: 0.594425 | Val Acc: 0.767232 loss: 0.743248\n",
      "[146/250] Train Acc: 0.802654 Loss: 0.592654 | Val Acc: 0.767276 loss: 0.744457\n",
      "[147/250] Train Acc: 0.804055 Loss: 0.589992 | Val Acc: 0.767618 loss: 0.743588\n",
      "[148/250] Train Acc: 0.803223 Loss: 0.591074 | Val Acc: 0.767232 loss: 0.743318\n",
      "[149/250] Train Acc: 0.803730 Loss: 0.591290 | Val Acc: 0.768150 loss: 0.742122\n",
      "saving model with acc 0.768\n",
      "[150/250] Train Acc: 0.803775 Loss: 0.589221 | Val Acc: 0.766764 loss: 0.745811\n",
      "[151/250] Train Acc: 0.803836 Loss: 0.589126 | Val Acc: 0.768325 loss: 0.744518\n",
      "saving model with acc 0.768\n",
      "[152/250] Train Acc: 0.804150 Loss: 0.588654 | Val Acc: 0.768293 loss: 0.745674\n",
      "[153/250] Train Acc: 0.804217 Loss: 0.588326 | Val Acc: 0.767756 loss: 0.744408\n",
      "[154/250] Train Acc: 0.804439 Loss: 0.586915 | Val Acc: 0.768134 loss: 0.745306\n",
      "[155/250] Train Acc: 0.804454 Loss: 0.586830 | Val Acc: 0.767626 loss: 0.743461\n",
      "[156/250] Train Acc: 0.804671 Loss: 0.586132 | Val Acc: 0.767374 loss: 0.743559\n",
      "[157/250] Train Acc: 0.804489 Loss: 0.586267 | Val Acc: 0.767553 loss: 0.743226\n",
      "[158/250] Train Acc: 0.805219 Loss: 0.585027 | Val Acc: 0.767683 loss: 0.742897\n",
      "[159/250] Train Acc: 0.805377 Loss: 0.584780 | Val Acc: 0.767236 loss: 0.747638\n",
      "[160/250] Train Acc: 0.805195 Loss: 0.585064 | Val Acc: 0.767967 loss: 0.747657\n",
      "[161/250] Train Acc: 0.805600 Loss: 0.583292 | Val Acc: 0.768073 loss: 0.743399\n",
      "[162/250] Train Acc: 0.806254 Loss: 0.581759 | Val Acc: 0.767394 loss: 0.745845\n",
      "[163/250] Train Acc: 0.806012 Loss: 0.582470 | Val Acc: 0.767081 loss: 0.745666\n",
      "[164/250] Train Acc: 0.806052 Loss: 0.582168 | Val Acc: 0.768008 loss: 0.745012\n",
      "[165/250] Train Acc: 0.805269 Loss: 0.583209 | Val Acc: 0.768789 loss: 0.745409\n",
      "saving model with acc 0.769\n",
      "[166/250] Train Acc: 0.806890 Loss: 0.579693 | Val Acc: 0.768016 loss: 0.746760\n",
      "[167/250] Train Acc: 0.807245 Loss: 0.578840 | Val Acc: 0.767675 loss: 0.747921\n",
      "[168/250] Train Acc: 0.806543 Loss: 0.579614 | Val Acc: 0.767715 loss: 0.746488\n",
      "[169/250] Train Acc: 0.807696 Loss: 0.578341 | Val Acc: 0.767687 loss: 0.745435\n",
      "[170/250] Train Acc: 0.807430 Loss: 0.578216 | Val Acc: 0.767992 loss: 0.746238\n",
      "[171/250] Train Acc: 0.807252 Loss: 0.577988 | Val Acc: 0.768614 loss: 0.746667\n",
      "[172/250] Train Acc: 0.807609 Loss: 0.577400 | Val Acc: 0.767695 loss: 0.746143\n",
      "[173/250] Train Acc: 0.807796 Loss: 0.576753 | Val Acc: 0.768276 loss: 0.745904\n",
      "[174/250] Train Acc: 0.808113 Loss: 0.575923 | Val Acc: 0.768000 loss: 0.747049\n",
      "[175/250] Train Acc: 0.807669 Loss: 0.575958 | Val Acc: 0.768089 loss: 0.747547\n",
      "[176/250] Train Acc: 0.808761 Loss: 0.573571 | Val Acc: 0.768411 loss: 0.748570\n",
      "[177/250] Train Acc: 0.808266 Loss: 0.574217 | Val Acc: 0.768313 loss: 0.748338\n",
      "[178/250] Train Acc: 0.808222 Loss: 0.573537 | Val Acc: 0.767870 loss: 0.747348\n",
      "[179/250] Train Acc: 0.808225 Loss: 0.573332 | Val Acc: 0.768402 loss: 0.747529\n",
      "[180/250] Train Acc: 0.809328 Loss: 0.572223 | Val Acc: 0.768012 loss: 0.748546\n",
      "[181/250] Train Acc: 0.808723 Loss: 0.572498 | Val Acc: 0.768081 loss: 0.748344\n",
      "[182/250] Train Acc: 0.808763 Loss: 0.573102 | Val Acc: 0.768793 loss: 0.745699\n",
      "saving model with acc 0.769\n",
      "[183/250] Train Acc: 0.808971 Loss: 0.571995 | Val Acc: 0.767784 loss: 0.746828\n",
      "[184/250] Train Acc: 0.809597 Loss: 0.570229 | Val Acc: 0.767520 loss: 0.750457\n",
      "[185/250] Train Acc: 0.809287 Loss: 0.570848 | Val Acc: 0.768463 loss: 0.748690\n",
      "[186/250] Train Acc: 0.809353 Loss: 0.569889 | Val Acc: 0.767740 loss: 0.746363\n",
      "[187/250] Train Acc: 0.809309 Loss: 0.569857 | Val Acc: 0.767837 loss: 0.748926\n",
      "[188/250] Train Acc: 0.809804 Loss: 0.569435 | Val Acc: 0.767130 loss: 0.751599\n",
      "[189/250] Train Acc: 0.810040 Loss: 0.567397 | Val Acc: 0.767691 loss: 0.750630\n",
      "[190/250] Train Acc: 0.809925 Loss: 0.568882 | Val Acc: 0.767183 loss: 0.749053\n",
      "[191/250] Train Acc: 0.810211 Loss: 0.567087 | Val Acc: 0.768272 loss: 0.750622\n",
      "[192/250] Train Acc: 0.810094 Loss: 0.569228 | Val Acc: 0.768053 loss: 0.750587\n",
      "[193/250] Train Acc: 0.810396 Loss: 0.567523 | Val Acc: 0.768411 loss: 0.749890\n",
      "[194/250] Train Acc: 0.810604 Loss: 0.566262 | Val Acc: 0.769520 loss: 0.747916\n",
      "saving model with acc 0.770\n",
      "[195/250] Train Acc: 0.811007 Loss: 0.566044 | Val Acc: 0.768382 loss: 0.750467\n",
      "[196/250] Train Acc: 0.810625 Loss: 0.566138 | Val Acc: 0.767971 loss: 0.748756\n",
      "[197/250] Train Acc: 0.811250 Loss: 0.565949 | Val Acc: 0.767691 loss: 0.747059\n",
      "[198/250] Train Acc: 0.811307 Loss: 0.564257 | Val Acc: 0.767228 loss: 0.749476\n",
      "[199/250] Train Acc: 0.811532 Loss: 0.564940 | Val Acc: 0.768175 loss: 0.751353\n",
      "[200/250] Train Acc: 0.811668 Loss: 0.563893 | Val Acc: 0.768110 loss: 0.745956\n",
      "[201/250] Train Acc: 0.811110 Loss: 0.564263 | Val Acc: 0.768683 loss: 0.748165\n",
      "[202/250] Train Acc: 0.811259 Loss: 0.564191 | Val Acc: 0.767719 loss: 0.749127\n",
      "[203/250] Train Acc: 0.812298 Loss: 0.561864 | Val Acc: 0.768427 loss: 0.748454\n",
      "[204/250] Train Acc: 0.811319 Loss: 0.562952 | Val Acc: 0.767752 loss: 0.750195\n",
      "[205/250] Train Acc: 0.812102 Loss: 0.561740 | Val Acc: 0.767671 loss: 0.751285\n",
      "[206/250] Train Acc: 0.812376 Loss: 0.560821 | Val Acc: 0.768089 loss: 0.753438\n",
      "[207/250] Train Acc: 0.812703 Loss: 0.562536 | Val Acc: 0.767858 loss: 0.749591\n",
      "[208/250] Train Acc: 0.812127 Loss: 0.560807 | Val Acc: 0.768321 loss: 0.750996\n",
      "[209/250] Train Acc: 0.812417 Loss: 0.560489 | Val Acc: 0.768293 loss: 0.752933\n",
      "[210/250] Train Acc: 0.812683 Loss: 0.559809 | Val Acc: 0.768167 loss: 0.752120\n",
      "[211/250] Train Acc: 0.812528 Loss: 0.560647 | Val Acc: 0.768850 loss: 0.749514\n",
      "[212/250] Train Acc: 0.812824 Loss: 0.560242 | Val Acc: 0.768106 loss: 0.752758\n",
      "[213/250] Train Acc: 0.812556 Loss: 0.560429 | Val Acc: 0.767967 loss: 0.748763\n",
      "[214/250] Train Acc: 0.813595 Loss: 0.557372 | Val Acc: 0.768378 loss: 0.753117\n",
      "[215/250] Train Acc: 0.812706 Loss: 0.559982 | Val Acc: 0.767776 loss: 0.750646\n",
      "[216/250] Train Acc: 0.813636 Loss: 0.556580 | Val Acc: 0.768646 loss: 0.751930\n",
      "[217/250] Train Acc: 0.813377 Loss: 0.558585 | Val Acc: 0.769171 loss: 0.749925\n",
      "[218/250] Train Acc: 0.813130 Loss: 0.556814 | Val Acc: 0.768451 loss: 0.750594\n",
      "[219/250] Train Acc: 0.813494 Loss: 0.557159 | Val Acc: 0.768472 loss: 0.750818\n",
      "[220/250] Train Acc: 0.813692 Loss: 0.556535 | Val Acc: 0.766906 loss: 0.754020\n",
      "[221/250] Train Acc: 0.814176 Loss: 0.554611 | Val Acc: 0.769146 loss: 0.751689\n",
      "[222/250] Train Acc: 0.814184 Loss: 0.555122 | Val Acc: 0.768740 loss: 0.752428\n",
      "[223/250] Train Acc: 0.813883 Loss: 0.555450 | Val Acc: 0.767984 loss: 0.749483\n",
      "[224/250] Train Acc: 0.813913 Loss: 0.555979 | Val Acc: 0.768549 loss: 0.748132\n",
      "[225/250] Train Acc: 0.814375 Loss: 0.553805 | Val Acc: 0.767963 loss: 0.750531\n",
      "[226/250] Train Acc: 0.814822 Loss: 0.553610 | Val Acc: 0.767959 loss: 0.752582\n",
      "[227/250] Train Acc: 0.814243 Loss: 0.555349 | Val Acc: 0.768244 loss: 0.751429\n",
      "[228/250] Train Acc: 0.814537 Loss: 0.552849 | Val Acc: 0.768012 loss: 0.753493\n",
      "[229/250] Train Acc: 0.814301 Loss: 0.553618 | Val Acc: 0.768923 loss: 0.752596\n",
      "[230/250] Train Acc: 0.814516 Loss: 0.553199 | Val Acc: 0.769159 loss: 0.750342\n",
      "[231/250] Train Acc: 0.814866 Loss: 0.552192 | Val Acc: 0.768699 loss: 0.750113\n",
      "[232/250] Train Acc: 0.814132 Loss: 0.552931 | Val Acc: 0.768394 loss: 0.748662\n",
      "[233/250] Train Acc: 0.814821 Loss: 0.552751 | Val Acc: 0.768667 loss: 0.750602\n",
      "[234/250] Train Acc: 0.815328 Loss: 0.549651 | Val Acc: 0.768158 loss: 0.753000\n",
      "[235/250] Train Acc: 0.815077 Loss: 0.551095 | Val Acc: 0.768512 loss: 0.756002\n",
      "[236/250] Train Acc: 0.815047 Loss: 0.551343 | Val Acc: 0.767894 loss: 0.754441\n",
      "[237/250] Train Acc: 0.815656 Loss: 0.550460 | Val Acc: 0.768232 loss: 0.753643\n",
      "[238/250] Train Acc: 0.815533 Loss: 0.550107 | Val Acc: 0.767723 loss: 0.756350\n",
      "[239/250] Train Acc: 0.815908 Loss: 0.549861 | Val Acc: 0.768439 loss: 0.750350\n",
      "[240/250] Train Acc: 0.815897 Loss: 0.549442 | Val Acc: 0.768707 loss: 0.753405\n",
      "[241/250] Train Acc: 0.815820 Loss: 0.550414 | Val Acc: 0.768032 loss: 0.753071\n",
      "[242/250] Train Acc: 0.815982 Loss: 0.549083 | Val Acc: 0.768764 loss: 0.752680\n",
      "[243/250] Train Acc: 0.815930 Loss: 0.548423 | Val Acc: 0.768207 loss: 0.756625\n",
      "[244/250] Train Acc: 0.815927 Loss: 0.548681 | Val Acc: 0.768862 loss: 0.754146\n",
      "[245/250] Train Acc: 0.816768 Loss: 0.548605 | Val Acc: 0.768492 loss: 0.753955\n",
      "[246/250] Train Acc: 0.815829 Loss: 0.547879 | Val Acc: 0.768988 loss: 0.752914\n",
      "[247/250] Train Acc: 0.816622 Loss: 0.547732 | Val Acc: 0.768866 loss: 0.755465\n",
      "[248/250] Train Acc: 0.816188 Loss: 0.546629 | Val Acc: 0.767858 loss: 0.754556\n",
      "[249/250] Train Acc: 0.816560 Loss: 0.547180 | Val Acc: 0.768870 loss: 0.752883\n",
      "[250/250] Train Acc: 0.816898 Loss: 0.546325 | Val Acc: 0.767825 loss: 0.755656\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "trainacc = []\n",
    "valacc = []\n",
    "best_acc = 0.0\n",
    "for epoch in range(num_epoch):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train() # set the model to training mode\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) #Moves the data to the specified device (e.g., GPU).\n",
    "        optimizer.zero_grad() # Clears the gradients of all optimized variables.\n",
    "        outputs = model(inputs) # Forward pass to obtain model predictions.\n",
    "        batch_loss = criterion(outputs, labels) #Computes the loss between model predictions and actual labels using the specified loss criterion.\n",
    "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability # max_values, indices = torch.max(outputs, 1)\n",
    "        batch_loss.backward() #Backward pass to compute gradients.\n",
    "        optimizer.step() #Updates the model parameters using the optimizer.\n",
    "\n",
    "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    # validation\n",
    "    if len(val_set) > 0:\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                batch_loss = criterion(outputs, labels) \n",
    "                _, val_pred = torch.max(outputs, 1) \n",
    "            \n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                # val_pred.cpu() == labels.cpu() Performs element-wise comparison between the two tensors, resulting in a boolean tensor indicating whether each element is equal.\n",
    "                # some operations (like tensor comparisons and sum) may not be supported when tensors are located on different devices (e.g., GPU and CPU). By calling .cpu(), you make sure that both tensors are on the CPU and can be used together.\n",
    "                # .sum(): Computes the sum of the boolean tensor. True is treated as 1, and False is treated as 0.\n",
    "                # .item(): Converts the sum to a Python scalar (integer). This is necessary because the result of the sum operation is a PyTorch tensor.\n",
    "                val_loss += batch_loss.item()\n",
    "\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
    "            ))\n",
    "            train_losses.append(train_loss/len(train_loader))\n",
    "            valid_losses.append(val_loss/len(val_loader))\n",
    "            trainacc.append(train_acc/len(train_set))\n",
    "            valacc.append(val_acc/len(val_set))\n",
    "\n",
    "            # if the model improves, save a checkpoint at this epoch\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
    "        ))\n",
    "\n",
    "# if not validating, save the last epoch\n",
    "if len(val_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Epoch': range(1, len(train_losses) + 1),\n",
    "    'Train_Loss': train_losses,\n",
    "    'Valid_Loss': valid_losses,\n",
    "    'Train_acc': trainacc,\n",
    "    'Val_acc': valacc\n",
    "})\n",
    "\n",
    "data.to_csv('final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create testing dataset\n",
    "test_set = TIMITDataset(test, None)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# create model and load weights from checkpoint\n",
    "model = Classifier().to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []\n",
    "model.eval() # set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs = data\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "\n",
    "        for y in test_pred.cpu().numpy():\n",
    "            predict.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction.csv', 'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i, y in enumerate(predict):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
