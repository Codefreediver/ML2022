{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_11.npy → training data (# of training frames, 11 x feature dim)\n",
    "train_label_11.npy → framewise phoneme label (0-38)\n",
    "test_11.npy → testing data (# of testing frames, 11 x feature dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Size of training data: (1229932, 429)\n",
      "Size of testing data: (451552, 429)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Loading data ...')\n",
    "\n",
    "data_root=\"C:\\\\Users\\\\Michael\\\\Desktop\\\\PYTHON\\\\Machine Learning\\\\HW2\\\\timit_11\\\\\"\n",
    "train = np.load(data_root + 'train_11.npy')\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "test = np.load(data_root + 'test_11.npy')\n",
    "\n",
    "print('Size of training data: {}'.format(train.shape))\n",
    "print('Size of testing data: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create DataSet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TIMITDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = torch.from_numpy(X).float()\n",
    "        if y is not None:\n",
    "            y = y.astype(int)\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliy Train and Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: (983945, 429)\n",
      "Size of validation set: (245987, 429)\n"
     ]
    }
   ],
   "source": [
    "VAL_RATIO = 0.2\n",
    "\n",
    "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
    "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
    "print('Size of training set: {}'.format(train_x.shape))\n",
    "print('Size of validation set: {}'.format(val_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = TIMITDataset(train_x, train_y)\n",
    "val_set = TIMITDataset(val_x, val_y)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False) # Why shuffle the training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del train, train_label, train_x, train_y, val_x, val_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(429, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 512)\n",
    "        self.layer3 = nn.Linear(512, 128)\n",
    "        self.out = nn.Linear(128, 39) \n",
    "\n",
    "        self.act_fn = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.act_fn(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check device\n",
    "def get_device():\n",
    "  return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "same_seeds(0)\n",
    "\n",
    "# get device \n",
    "device = get_device()\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "# training parameters\n",
    "num_epoch = 20               # number of training epoch\n",
    "learning_rate = 0.0001       # learning rate\n",
    "\n",
    "# the path where checkpoint saved\n",
    "model_path = './model.ckpt'\n",
    "\n",
    "# create model, define a loss function, and optimizer\n",
    "model = Classifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/020] Train Acc: 0.467302 Loss: 1.811661 | Val Acc: 0.567428 loss: 1.433065\n",
      "saving model with acc 0.567\n",
      "[002/020] Train Acc: 0.594383 Loss: 1.330665 | Val Acc: 0.628639 loss: 1.211098\n",
      "saving model with acc 0.629\n",
      "[003/020] Train Acc: 0.644506 Loss: 1.154064 | Val Acc: 0.660421 loss: 1.101215\n",
      "saving model with acc 0.660\n",
      "[004/020] Train Acc: 0.672216 Loss: 1.052246 | Val Acc: 0.676300 loss: 1.038718\n",
      "saving model with acc 0.676\n",
      "[005/020] Train Acc: 0.691348 Loss: 0.983103 | Val Acc: 0.685154 loss: 1.001852\n",
      "saving model with acc 0.685\n",
      "[006/020] Train Acc: 0.705616 Loss: 0.931955 | Val Acc: 0.689301 loss: 0.984177\n",
      "saving model with acc 0.689\n",
      "[007/020] Train Acc: 0.716345 Loss: 0.891687 | Val Acc: 0.694516 loss: 0.964627\n",
      "saving model with acc 0.695\n",
      "[008/020] Train Acc: 0.725881 Loss: 0.857907 | Val Acc: 0.697720 loss: 0.951889\n",
      "saving model with acc 0.698\n",
      "[009/020] Train Acc: 0.733718 Loss: 0.829495 | Val Acc: 0.696687 loss: 0.949866\n",
      "[010/020] Train Acc: 0.741151 Loss: 0.803701 | Val Acc: 0.699378 loss: 0.944832\n",
      "saving model with acc 0.699\n",
      "[011/020] Train Acc: 0.748049 Loss: 0.781106 | Val Acc: 0.697773 loss: 0.946494\n",
      "[012/020] Train Acc: 0.753792 Loss: 0.760380 | Val Acc: 0.702830 loss: 0.938236\n",
      "saving model with acc 0.703\n",
      "[013/020] Train Acc: 0.759404 Loss: 0.741234 | Val Acc: 0.700456 loss: 0.945627\n",
      "[014/020] Train Acc: 0.764573 Loss: 0.723574 | Val Acc: 0.702159 loss: 0.942118\n",
      "[015/020] Train Acc: 0.769472 Loss: 0.707325 | Val Acc: 0.704427 loss: 0.936154\n",
      "saving model with acc 0.704\n",
      "[016/020] Train Acc: 0.773688 Loss: 0.691314 | Val Acc: 0.701736 loss: 0.945713\n",
      "[017/020] Train Acc: 0.778676 Loss: 0.676633 | Val Acc: 0.701586 loss: 0.953081\n",
      "[018/020] Train Acc: 0.783108 Loss: 0.662425 | Val Acc: 0.699663 loss: 0.963290\n",
      "[019/020] Train Acc: 0.786396 Loss: 0.649180 | Val Acc: 0.700082 loss: 0.957681\n",
      "[020/020] Train Acc: 0.790643 Loss: 0.636623 | Val Acc: 0.699736 loss: 0.964269\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "trainacc = []\n",
    "valacc = []\n",
    "best_acc = 0.0\n",
    "for epoch in range(num_epoch):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train() # set the model to training mode\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) #Moves the data to the specified device (e.g., GPU).\n",
    "        optimizer.zero_grad() # Clears the gradients of all optimized variables.\n",
    "        outputs = model(inputs) # Forward pass to obtain model predictions.\n",
    "        batch_loss = criterion(outputs, labels) #Computes the loss between model predictions and actual labels using the specified loss criterion.\n",
    "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability # max_values, indices = torch.max(outputs, 1)\n",
    "        batch_loss.backward() #Backward pass to compute gradients.\n",
    "        optimizer.step() #Updates the model parameters using the optimizer.\n",
    "\n",
    "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    # validation\n",
    "    if len(val_set) > 0:\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                batch_loss = criterion(outputs, labels) \n",
    "                _, val_pred = torch.max(outputs, 1) \n",
    "            \n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                # val_pred.cpu() == labels.cpu() Performs element-wise comparison between the two tensors, resulting in a boolean tensor indicating whether each element is equal.\n",
    "                # some operations (like tensor comparisons and sum) may not be supported when tensors are located on different devices (e.g., GPU and CPU). By calling .cpu(), you make sure that both tensors are on the CPU and can be used together.\n",
    "                # .sum(): Computes the sum of the boolean tensor. True is treated as 1, and False is treated as 0.\n",
    "                # .item(): Converts the sum to a Python scalar (integer). This is necessary because the result of the sum operation is a PyTorch tensor.\n",
    "                val_loss += batch_loss.item()\n",
    "\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
    "            ))\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(val_loss)\n",
    "            trainacc.append(train_acc)\n",
    "            valacc.append(val_acc)\n",
    "\n",
    "            # if the model improves, save a checkpoint at this epoch\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
    "        ))\n",
    "\n",
    "# if not validating, save the last epoch\n",
    "if len(val_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3897559774.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    'Valid_Loss': valid_losses\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({\n",
    "    'Epoch': range(1, len(train_losses) + 1),\n",
    "    'Train_Loss': train_losses\n",
    "    'Valid_Loss': valid_losses\n",
    "    \"Train_acc\": trainacc\n",
    "    'Val_acc': valacc    \n",
    "})\n",
    "\n",
    "data.to_csv('testing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
